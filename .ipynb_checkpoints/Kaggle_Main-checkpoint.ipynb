{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "from logisticClassify2 import *\n",
    "from sklearn import ensemble as sk_ensemble\n",
    "from sklearn import tree\n",
    "from sklearn import neural_network\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X_test = np.genfromtxt(\"data/X_test.txt\",delimiter=None)\n",
    "X = np.genfromtxt(\"data/X_train.txt\",delimiter=None)\n",
    "Y = np.genfromtxt(\"data/Y_train.txt\",delimiter=None)\n",
    "\n",
    "X,Y = ml.shuffleData(X,Y)\n",
    "# X = X[:20000]\n",
    "# Y = Y[:20000]\n",
    "[Xtr,Xva,Ytr,Yva] = ml.splitData(X,Y,0.75)\n",
    "train_shape = Xtr.shape[0]\n",
    "\n",
    "learner_list = []\n",
    "\n",
    "x,y = ml.bootstrapData(Xtr, Ytr, train_shape)\n",
    "x_val,y_val = ml.bootstrapData(Xva, Yva, train_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [ 25000 100000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-40019e49213f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[1;31m# YvaHat = knn.predict(X_va)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AUC for KNN is {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_va\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYvaHat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[1;31m# print(\"Accuracy Score for Adaboost is {}\".format(accuracy_score(Y_va, YvaHat)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[1;31m# knn.fit(X_va, Y_va)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Juston\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    255\u001b[0m     return _average_binary_score(\n\u001b[1;32m    256\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Juston\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Juston\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         fpr, tpr, tresholds = roc_curve(y_true, y_score,\n\u001b[0;32m--> 252\u001b[0;31m                                         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreorder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Juston\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \"\"\"\n\u001b[1;32m    500\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 501\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[1;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Juston\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mDecreasing\u001b[0m \u001b[0mscore\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \"\"\"\n\u001b[0;32m--> 294\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Juston\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[0;32m--> 176\u001b[0;31m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [ 25000 100000]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "import itertools\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def preprocess(X_tr, X_va, n_features=None, weights=None, normalize=True):\n",
    "    if not weights:\n",
    "        model = ExtraTreesClassifier()\n",
    "        model.fit(X_tr, Y_tr)\n",
    "        percentages = model.feature_importances_\n",
    "        max_p = max(percentages)\n",
    "        weights = np.array([p/max_p for p in percentages])\n",
    "    if normalize:\n",
    "        X_tr = (X_tr - X_tr.min(axis=0))/(X_tr.max(axis=0))\n",
    "        X_va = (X_va - X_va.min(axis=0))/(X_va.max(axis=0))\n",
    "    X_tr *= weights\n",
    "    X_va *= weights\n",
    "    if n_features:\n",
    "        pca = PCA(n_components=n_features)\n",
    "        pca.fit(X_tr)\n",
    "        X_tr = pca.transform(X_tr)\n",
    "        pca.fit(X_va)\n",
    "        X_va = pca.transform(X_va)\n",
    "    return X_tr, X_va\n",
    "\n",
    "X_te = np.genfromtxt(\"data/X_test.txt\",delimiter=None)\n",
    "X_tr = np.genfromtxt(\"data/X_train.txt\",delimiter=None)\n",
    "Y_tr = np.genfromtxt(\"data/Y_train.txt\",delimiter=None)\n",
    "\n",
    "X_tr, X_va, Y_tr, Y_va = ml.splitData(X_tr, Y_tr, 0.75)\n",
    "\n",
    "\n",
    "# print(\"Original X_tr\")\n",
    "# print(\"---------------------------------------------------------------------------\")\n",
    "# print(\"Number of Observations: {}\".format(X_tr.shape[0]))\n",
    "# print(\"Number of features: {}\\n\".format(X_tr.shape[1]))\n",
    "# print(X_tr)\n",
    "# print()\n",
    "\n",
    "#no preprocessing at all gets the highest auc score for k = 15\n",
    "\n",
    "# #no preprocessing at all\n",
    "# #k_list = [10, 15, 55, 100, 500, 1000, 3000]\n",
    "# k_list = [15]\n",
    "# for k in k_list:\n",
    "    \n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(X_tr, Y_tr)\n",
    "YvaHat = knn.predict(X_va)\n",
    "print(\"AUC for KNN is {}\".format(roc_auc_score(Y_va, YvaHat)))\n",
    "# print(\"Accuracy Score for Adaboost is {}\".format(accuracy_score(Y_va, YvaHat)))\n",
    "# knn.fit(X_va, Y_va)\n",
    "# YvaHat = knn.predict(X_tr)\n",
    "# print()\n",
    "YvaHat = knn.predict(X_test)\n",
    "learner_list.append(YvaHat)\n",
    "\n",
    "\n",
    "# #with full on preprocessing\n",
    "# k_list = [400, 500]\n",
    "# for n in [2, 3, 4]:\n",
    "#     print(\"number of features: {}\".format(n))\n",
    "#     X_tr_prime, X_va_prime = preprocess(X_tr, X_va, n_features=n)\n",
    "#     X_tr_prime, X_te_prime = preprocess(X_tr, X_te, n_features=n)\n",
    "#     print(\"X_tr post-preprocessing\")\n",
    "#     print(X_tr_prime)\n",
    "#     print()\n",
    "#     for k in k_list:\n",
    "#         knn = KNeighborsRegressor(n_neighbors=k)\n",
    "#         knn.fit(X_tr_prime, Y_tr)\n",
    "#         YvaHat = knn.predict(X_va_prime)\n",
    "#         print(\"For k={}, AUC is {}\".format(k, roc_auc_score(Y_va, YvaHat)))\n",
    "#         YvaHat = knn.predict(X_te_prime)\n",
    "#         learner_list.append(YvaHat)\n",
    "#     print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPrevious scores:\\nAdaBoostClassifier() = 0.680875\\nAdaBoostClassifier(n_estimators = 100) = 0.6795625\\nAdaBoostClassifier(n_estimators = 100, learning_rate = 10) = 0.344375\\nAdaBoostClassifier(n_estimators = 100, learning_rate = 0.5) = 0.681\\nAdaBoostClassifier(n_estimators = 100, learning_rate = 0.1) = 0.6771875\\nAdaBoostClassifier(n_estimators = 25, learning_rate = 0.5) = 0.675875\\nAdaBoostClassifier(base_estimator = tree.ExtraTreeClassifier(),n_estimators = 100, learning_rate = 0.5) = 0.6735625\\nAdaBoostClassifier(base_estimator = tree.DecisionTreeClassifier(),n_estimators = 100, learning_rate = 0.5) = 0.681375\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adabooster:\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ada_clf = AdaBoostRegressor(base_estimator = tree.ExtraTreeClassifier(),n_estimators = 100, learning_rate = 0.5)\n",
    "ada_clf.fit(X_tr, Y_tr)\n",
    "# ada_predictions = ada_clf.predict(X_va)\n",
    "# print(\"AUC for Adaboost is {}\".format(roc_auc_score(Y_va, ada_predictions)))\n",
    "# print(\"MSE_validation for Adaboost is {}\".format(mean_squared_error(Y_va, ada_predictions)))\n",
    "# ada_clf.fit(X_va, Y_va)\n",
    "# ada_predictions = ada_clf.predict(X_tr)\n",
    "# print(\"MSE_train for Adaboost is {}\".format(mean_squared_error(Y_tr, ada_predictions)))\n",
    "# print()\n",
    "ada_predictions = ada_clf.predict(X_test)\n",
    "learner_list.append(ada_predictions)\n",
    "'''\n",
    "Previous scores:\n",
    "AdaBoostClassifier() = 0.680875\n",
    "AdaBoostClassifier(n_estimators = 100) = 0.6795625\n",
    "AdaBoostClassifier(n_estimators = 100, learning_rate = 10) = 0.344375\n",
    "AdaBoostClassifier(n_estimators = 100, learning_rate = 0.5) = 0.681\n",
    "AdaBoostClassifier(n_estimators = 100, learning_rate = 0.1) = 0.6771875\n",
    "AdaBoostClassifier(n_estimators = 25, learning_rate = 0.5) = 0.675875\n",
    "AdaBoostClassifier(base_estimator = tree.ExtraTreeClassifier(),n_estimators = 100, learning_rate = 0.5) = 0.6735625\n",
    "AdaBoostClassifier(base_estimator = tree.DecisionTreeClassifier(),n_estimators = 100, learning_rate = 0.5) = 0.681375\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nList of Previous Scores:\\nRandomForestClassifier(n_estimators = 50, max_depth = 15, min_samples_leaf = 2**7) == 0.6873125\\nRandomForestClassifier(n_estimators = 50, max_depth = 30, min_samples_leaf = 2**7) == 0.692875\\nRandomForestClassifier(n_estimators = 50, max_depth = 50, min_samples_leaf = 2**7) == 0.692875\\nRandomForestClassifier(n_estimators = 50, max_depth = 30, min_samples_leaf = 2**10) == 0.6818125\\nRandomForestClassifier(n_estimators = 100, max_depth = 30, min_samples_leaf = 2**7) == 0.6890625\\n.RandomForestClassifier(n_estimators = 10, max_depth = 30, min_samples_leaf = 2**7) == 0.6868125\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Linear regression\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
    "Don't know if any other model would work\n",
    "'''\n",
    "\n",
    "#Random Forest Classifier:\n",
    "x,y = ml.bootstrapData(Xtr, Ytr, train_shape)\n",
    "x_val,y_val = ml.bootstrapData(Xva, Yva, train_shape)\n",
    "dtree = sk_ensemble.RandomForestClassifier(n_estimators = 50, max_depth = 50, min_samples_leaf = 2**3)\n",
    "dtree.fit(X_tr, Y_tr)\n",
    "# tree_predict = dtree.predict(X_va)\n",
    "# print(\"AUC for tree_predict is {}\".format(roc_auc_score(Y_va, tree_predict)))\n",
    "# print(\"MSE_validation for Random Forest is {}\".format(mean_squared_error(Y_va, tree_predict)))\n",
    "# dtree.fit(X_va, Y_va)\n",
    "# tree_predict = dtree.predict(X_tr)\n",
    "# print(\"MSE_train for Random Forest is {}\".format(mean_squared_error(Y_tr, tree_predict)))\n",
    "# print()\n",
    "tree_predict = dtree.predict(X_test)\n",
    "learner_list.append(tree_predict)\n",
    "'''\n",
    "List of Previous Scores:\n",
    "RandomForestClassifier(n_estimators = 50, max_depth = 15, min_samples_leaf = 2**7) == 0.6873125\n",
    "RandomForestClassifier(n_estimators = 50, max_depth = 30, min_samples_leaf = 2**7) == 0.692875\n",
    "RandomForestClassifier(n_estimators = 50, max_depth = 50, min_samples_leaf = 2**7) == 0.692875\n",
    "RandomForestClassifier(n_estimators = 50, max_depth = 30, min_samples_leaf = 2**10) == 0.6818125\n",
    "RandomForestClassifier(n_estimators = 100, max_depth = 30, min_samples_leaf = 2**7) == 0.6890625\n",
    ".RandomForestClassifier(n_estimators = 10, max_depth = 30, min_samples_leaf = 2**7) == 0.6868125\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "100000\n",
      "[[ 0.5  0.3  0.5 ...,  0.4  0.5  0. ]\n",
      " [ 1.   0.   0.  ...,  0.   1.   0. ]\n",
      " [ 0.   0.   0.  ...,  1.   0.   0. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCurrent Final Scores:\\n0.62284\\n0.62678\\n0.59 %either ditch adaregressor or improve it\\n0.64604\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final Blending Method:\n",
    "Ypred = []\n",
    "review_list = learner_list\n",
    "for each_list in review_list:\n",
    "    print(len(each_list))\n",
    "new_learner_list = np.array(learner_list)\n",
    "print(new_learner_list)\n",
    "for each_item in range(len(new_learner_list[0])):\n",
    "    Ypred.append(int(np.mean(new_learner_list[:, each_item]) > 0.5))\n",
    "\n",
    "    \n",
    "# print(\"MSE_validation for Ensemble is {}\".format(mean_squared_error(Y_tr, Ypred)))\n",
    "# Now output a file with two columns, a row ID and a confidence in class 1:\n",
    "np.savetxt('Yhat.txt', np.vstack( (np.arange(len(Ypred)) , Ypred[:]) ).T, '%d, %.2f',header='ID,Prob1',comments='',delimiter=',');\n",
    "\n",
    "'''\n",
    "Current Final Scores:\n",
    "0.62284\n",
    "0.62678\n",
    "0.59 %either ditch adaregressor or improve it\n",
    "0.64604\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
